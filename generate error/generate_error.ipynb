{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8cf48810-ca73-4a42-9760-9bdd6fb0344b",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './tests.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 235\u001b[0m\n\u001b[0;32m    233\u001b[0m                 \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msentence\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mnew_sentence\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mposition\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    234\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 235\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[2], line 215\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    212\u001b[0m input_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./tests.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    213\u001b[0m output_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./output.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# 指定输出文件\u001b[39;00m\n\u001b[1;32m--> 215\u001b[0m sentences, masks, characters \u001b[38;5;241m=\u001b[39m \u001b[43mload_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    216\u001b[0m load_words(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./words\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    217\u001b[0m pinyin2chs \u001b[38;5;241m=\u001b[39m generate_set(characters)\n",
      "Cell \u001b[1;32mIn[2], line 37\u001b[0m, in \u001b[0;36mload_data\u001b[1;34m(file_path)\u001b[0m\n\u001b[0;32m     35\u001b[0m masks \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     36\u001b[0m characters \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m---> 37\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mreadlines():\n\u001b[0;32m     38\u001b[0m     fields \u001b[38;5;241m=\u001b[39m line\u001b[38;5;241m.\u001b[39mstrip()\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     39\u001b[0m     sentence \u001b[38;5;241m=\u001b[39m fields[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32me:\\Anaconda\\envs\\py312\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    322\u001b[0m     )\n\u001b[1;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './tests.txt'"
     ]
    }
   ],
   "source": [
    "'''\n",
    "把文件路径啥的改了，没下的库下了就行，直接运行就能保存\n",
    "用来跑的数据要用html格式来分开，输出之后每段最后的数字是错误出现的地方\n",
    "\n",
    "'''\n",
    "\n",
    "import tools\n",
    "import sys\n",
    "import random\n",
    "from pypinyin import pinyin, Style\n",
    "import math\n",
    "import json\n",
    "#from bert import tokenization\n",
    "import multiprocessing\n",
    "\n",
    "\n",
    "frequency = [0]*50000\n",
    "\n",
    "white_tag = [\"PER\", \"ORG\", \"LOC\", \"nw\", \"nz\"]\n",
    "def parse_lac_result(lac_result_str):\n",
    "    lac_result = eval(lac_result_str)\n",
    "    tags = lac_result[\"tag\"]\n",
    "    words = lac_result[\"word\"]\n",
    "    mask = []\n",
    "    left = right = 0\n",
    "    for index in range(len(tags)):\n",
    "        right = left + len(words[index])\n",
    "        if tags[index] in white_tag:\n",
    "            mask.append((left, right))\n",
    "        left = right\n",
    "    return mask\n",
    "\n",
    "def load_data(file_path):\n",
    "    sentences = []\n",
    "    masks = []\n",
    "    characters = []\n",
    "    for line in open(file_path, \"r\", encoding='utf-8').readlines():\n",
    "        fields = line.strip().split('\\t')\n",
    "        sentence = fields[0]\n",
    "        sentences.append(sentence.strip())\n",
    "        mask = []\n",
    "        if len(fields) > 1:\n",
    "            mask = parse_lac_result(fields[1])\n",
    "\n",
    "        masks.append(mask)\n",
    "        for ch in sentence:\n",
    "            if tools.is_chinese(ch) == False:\n",
    "                continue\n",
    "            frequency[ord(ch)-0x4e00] += 1\n",
    "    #3000常用汉字\n",
    "    freq_val = []\n",
    "    for index in range(len(frequency)):\n",
    "        k = frequency[index]\n",
    "        if k > 0:\n",
    "            characters.append(chr(index+0x4e00))\n",
    "            freq_val.append(k)\n",
    "    freq_val.sort(reverse=True)\n",
    "    threshold = freq_val[1000]\n",
    "    for k in range(len(frequency)):\n",
    "        if frequency[k] > threshold:\n",
    "            frequency[k] = threshold\n",
    "\n",
    "    return sentences, masks, characters\n",
    "\n",
    "after = {}\n",
    "before = {}\n",
    "def load_words(file_path):\n",
    "    global after, before\n",
    "    for line in open(file_path, encoding='utf-8').readlines():\n",
    "        word = line.strip()\n",
    "        for index in range(1, len(word)):\n",
    "            ch = word[index]\n",
    "            collect = []\n",
    "            if ch in before:\n",
    "                collect = before[ch]\n",
    "            if word[index-1] not in collect:\n",
    "                collect.append(word[index-1])\n",
    "                before[ch] = collect\n",
    "        for index in range(0, len(word)-1):\n",
    "            ch = word[index]\n",
    "            collect = []\n",
    "            if ch in after:\n",
    "                collect = after[ch]\n",
    "            if word[index+1] not in collect:\n",
    "                collect.append(word[index+1])\n",
    "                after[ch] = collect\n",
    "\n",
    "def similar(ch, prefix, suffix):\n",
    "    if tools.is_chinese(ch)==False:\n",
    "        return \"\"\n",
    "    res = pinyin(ch, style=Style.TONE3, heteronym=False)\n",
    "    pys = []\n",
    "    for item in res:\n",
    "        for py in item:\n",
    "            if tools.is_chinese(py)==False and py not in pys:\n",
    "                pys.append(py)\n",
    "    if len(pys) == 0:\n",
    "        return \"\"\n",
    "    if len(pys) == 1:\n",
    "        py = pys[0]\n",
    "    else:\n",
    "        py = pys[random.randint(0, len(pys)-1)]\n",
    "    sim_set = similar_pinyin[py]\n",
    "    t = list(zip(*sim_set))\n",
    "    pys = t[0]\n",
    "    distances = t[1]\n",
    "    index = tools.random_choice(distances)\n",
    "    sim_py = pys[index]\n",
    "    chs = pinyin2chs[sim_py]\n",
    "    chs = remove(chs, ch)\n",
    "    if len(chs) == 0:\n",
    "        return \"\"\n",
    "    return choose_by_frequency(chs, prefix, suffix)\n",
    "\n",
    "def remove(collection, target):\n",
    "    result = []\n",
    "    for item in collection:\n",
    "        if item == target:\n",
    "            continue\n",
    "        result.append(item)\n",
    "    return result\n",
    "\n",
    "def choose_by_frequency(characters, prefix, suffix):\n",
    "    high_risk = []\n",
    "    if prefix in after:\n",
    "        high_risk.extend(after[prefix])\n",
    "    if suffix in before:\n",
    "        high_risk.extend(before[suffix])\n",
    "    freqs = []\n",
    "    for ch in characters:\n",
    "        bonus = 1\n",
    "        if ch in high_risk:\n",
    "            bonus = 2\n",
    "        freqs.append(frequency[ord(ch)-0x4e00]*bonus)\n",
    "        \n",
    "    index = tools.random_choice(freqs)\n",
    "    return characters[index]\n",
    "\n",
    "def is_masked(mask, pos):\n",
    "    for left, right in mask:\n",
    "        if pos >= left and pos < right:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "white_list = [\"他\", \"她\", \"它\"]\n",
    "class SimpleTokenizer:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "        \n",
    "    def tokenize(self, text):\n",
    "        \"\"\"\n",
    "        简单的中文分字，每个字作为一个token\n",
    "        \"\"\"\n",
    "        return list(text)\n",
    "\n",
    "# 替换原来的 tokenizer 初始化\n",
    "tokenizer = SimpleTokenizer()\n",
    "\n",
    "def replace(sentence, mask):\n",
    "    tokens = tokenizer.tokenize(sentence)\n",
    "    origin_index = list(range(len(tokens)))  # 简化索引计算\n",
    "    \n",
    "    if len(tokens) == 0:\n",
    "        return \"\", -1\n",
    "        \n",
    "    position = min(int(math.floor(random.random() * len(tokens))), len(tokens)-1)\n",
    "    if is_masked(mask, position):  # 使用position替代origin_index[position]\n",
    "        return (\"\".join(tokens), -1)\n",
    "        \n",
    "    prefix = \"\"\n",
    "    suffix = \"\"\n",
    "    if position > 0:\n",
    "        prefix = tokens[position-1]\n",
    "    if position < len(tokens)-1:\n",
    "        suffix = tokens[position+1]\n",
    "        \n",
    "    sim_ch = similar(tokens[position], prefix, suffix)\n",
    "    \n",
    "    if sim_ch == \"\" or sim_ch in white_list or sim_ch == tokens[position]:\n",
    "        return (\"\".join(tokens), -1)\n",
    "        \n",
    "    tokens[position] = sim_ch\n",
    "    return (\"\".join(tokens), position)\n",
    "def build_similar_set(phonetics):\n",
    "    similar_pinyin = {}\n",
    "    for a in phonetics:\n",
    "        sim_set = []\n",
    "        for b in phonetics:\n",
    "            dis = tools.pinyin_distance(a, b)\n",
    "            if dis < 10:\n",
    "                sim_set.append((b, 1/(dis+1)))\n",
    "        similar_pinyin[a] = sim_set\n",
    "    return similar_pinyin\n",
    "\n",
    "def generate_set(characters):\n",
    "    pinyin2chs = {}\n",
    "    for ch in characters:\n",
    "        pys = pinyin(ch, style=Style.TONE3, heteronym=False)\n",
    "        for item in pys:\n",
    "            for py in item:\n",
    "                if py not in pinyin2chs:\n",
    "                    pinyin2chs[py] = [ch]\n",
    "                else:\n",
    "                    pinyin2chs[py].append(ch)\n",
    "    return pinyin2chs\n",
    "\n",
    "pinyin2chs = {}\n",
    "similar_pinyin = {}\n",
    "\n",
    "def main():\n",
    "    global pinyin2chs, similar_pinyin\n",
    "    input_file = \"./tests\"\n",
    "    output_file = \"./output.txt\"  # 指定输出文件\n",
    "    \n",
    "    sentences, masks, characters = load_data(input_file)\n",
    "    load_words(\"./words\")\n",
    "    pinyin2chs = generate_set(characters)\n",
    "    similar_pinyin = build_similar_set(pinyin2chs.keys())\n",
    "    \n",
    "    # 打开文件用于写入\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        for index in range(len(sentences)):\n",
    "            sentence = sentences[index]\n",
    "            mask = masks[index]\n",
    "            new_sentence, position = replace(sentence, mask)\n",
    "            if len(new_sentence) > 0 and position >= 0:\n",
    "                # 写入文件：原句\\t新句\\t替换位置\n",
    "                f.write(f\"错误文本： {new_sentence}\\n\")\n",
    "                f.write(f\"正确文本： {sentence}\\n\")\n",
    "                #f.write(f\"错误文本： {new_sentence}\\t{position}\\n\")\n",
    "                #f.write(f\"正确文本： {sentence}\\t{position}\\n\")\n",
    "                # 同时打印出来以便观察\n",
    "                print(f\"{sentence}\\t{new_sentence}\\t{position}\")\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e88def-1943-46d6-88f8-1fbb91849cbf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
